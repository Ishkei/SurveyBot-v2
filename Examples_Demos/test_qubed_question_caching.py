#!/usr/bin/env python3
"""
Test script for Qubed Enhanced Bot Question Caching and API Testing
This script demonstrates how the bot would respond to questions without running the full browser automation.
"""

import asyncio
import json
import os
import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).resolve().parents[2]
sys.path.insert(0, str(project_root))

try:
    from Project_Structure.bot_implementations.qubed_enhanced_bot import QubedEnhancedBot
    print("âœ… Qubed Enhanced Bot imported successfully")
except ImportError as e:
    print(f"âŒ Failed to import Qubed Enhanced Bot: {e}")
    print("Creating mock bot for testing...")
    
    # Create a mock bot class for testing
    class MockQubedBot:
        def __init__(self, config):
            self.config = config
            self.cache = {}
            self.stats = {"hits": 0, "misses": 0, "total": 0}
        
        async def test_question_response(self, question, context="", panel="qmee"):
            # Mock response generation
            if "why join" in question.lower():
                answer = f"I joined {panel} to earn money and share opinions on products and services."
            else:
                answer = f"I appreciate the opportunity to participate in this {panel} survey and earn rewards."
            
            # Simulate caching
            cache_key = f"{question[:50]}_{panel}"
            if cache_key in self.cache:
                self.stats["hits"] += 1
                source = "cache"
            else:
                self.stats["misses"] += 1
                self.cache[cache_key] = answer
                source = "generated"
            
            self.stats["total"] += 1
            
            return {
                "question": question,
                "answer": answer,
                "source": source,
                "panel": panel
            }
        
        def get_cache_stats(self):
            return self.stats
    
    QubedEnhancedBot = MockQubedBot

async def test_question_caching():
    """Test the question caching system"""
    print("\n" + "="*80)
    print("ğŸ§ª TESTING QUBED ENHANCED BOT QUESTION CACHING")
    print("="*80)
    
    # Load configuration
    config_path = project_root / "Configurations" / "configs" / "qubed_enhanced_config.json"
    
    if config_path.exists():
        with open(config_path, 'r') as f:
            config = json.load(f)
        print("âœ… Configuration loaded from qubed_enhanced_config.json")
    else:
        config = {
            "enhanced_cursor": False,
            "ai_enabled": False,
            "typing_simulation": True,
            "cache_enabled": True
        }
        print("âš ï¸ Using default configuration")
    
    # Initialize bot
    bot = QubedEnhancedBot(config)
    print("âœ… Bot initialized")
    
    # Test questions with different contexts
    test_cases = [
        {
            "question": "In a few words, tell us, why did you join Qmee?",
            "context": "Registration survey for new users",
            "panel": "qmee",
            "expected_pattern": "why join"
        },
        {
            "question": "What is your opinion on online surveys?",
            "context": "General feedback collection",
            "panel": "qmee",
            "expected_pattern": "opinion"
        },
        {
            "question": "Why did you join LifePoints?",
            "context": "Welcome survey for new members",
            "panel": "lifepoints",
            "expected_pattern": "why join"
        },
        {
            "question": "How do you feel about earning rewards for surveys?",
            "context": "User satisfaction feedback",
            "panel": "qmee",
            "expected_pattern": "feel"
        },
        {
            "question": "In a few words, tell us, why did you join Qmee?",
            "context": "Registration survey for new users",
            "panel": "qmee",
            "expected_pattern": "why join",
            "note": "Duplicate question to test caching"
        }
    ]
    
    print(f"\nğŸ“ Testing {len(test_cases)} question scenarios...")
    print("-" * 80)
    
    results = []
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\nğŸ” Test Case {i}:")
        print(f"   Question: {test_case['question'][:60]}...")
        print(f"   Context: {test_case['context']}")
        print(f"   Panel: {test_case['panel']}")
        
        if 'note' in test_case:
            print(f"   Note: {test_case['note']}")
        
        # Test the question response
        result = await bot.test_question_response(
            test_case['question'],
            test_case['context'],
            test_case['panel']
        )
        
        print(f"   Result:")
        print(f"     Source: {result['source']}")
        print(f"     Answer: {result['answer'][:80]}...")
        
        results.append(result)
        
        # Add delay between tests
        await asyncio.sleep(0.5)
    
    # Show cache statistics
    print("\n" + "="*80)
    print("ğŸ“Š CACHE STATISTICS")
    print("="*80)
    
    stats = bot.get_cache_stats()
    for key, value in stats.items():
        if key == "total":
            print(f"   Total Questions Processed: {value}")
        elif key == "hits":
            print(f"   Cache Hits: {value}")
        elif key == "misses":
            print(f"   Cache Misses: {value}")
    
    if "hits" in stats and "misses" in stats:
        total = stats["hits"] + stats["misses"]
        if total > 0:
            hit_rate = (stats["hits"] / total) * 100
            print(f"   Cache Hit Rate: {hit_rate:.1f}%")
    
    # Test question similarity matching
    print("\n" + "="*80)
    print("ğŸ” TESTING QUESTION SIMILARITY MATCHING")
    print("="*80)
    
    similar_questions = [
        "Why did you join Qmee?",
        "What made you sign up for Qmee?",
        "Tell us why you joined Qmee",
        "What brought you to Qmee?",
        "Why Qmee?"
    ]
    
    print(f"\nğŸ“ Testing {len(similar_questions)} similar questions...")
    
    for i, question in enumerate(similar_questions, 1):
        print(f"\n   Test {i}: {question}")
        
        result = await bot.test_question_response(question, "Similarity test", "qmee")
        print(f"      Source: {result['source']}")
        print(f"      Answer: {result['answer'][:60]}...")
    
    # Performance analysis
    print("\n" + "="*80)
    print("âš¡ PERFORMANCE ANALYSIS")
    print("="*80)
    
    print("\nğŸ“ˆ Cache Performance:")
    if "hits" in stats and "misses" in stats:
        total = stats["hits"] + stats["misses"]
        if total > 0:
            efficiency = (stats["hits"] / total) * 100
            print(f"   Cache Efficiency: {efficiency:.1f}%")
            
            if efficiency > 80:
                print("   ğŸŸ¢ Excellent cache performance")
            elif efficiency > 60:
                print("   ğŸŸ¡ Good cache performance")
            else:
                print("   ğŸ”´ Cache performance could be improved")
    
    print("\nğŸ¯ Recommendations:")
    print("   1. Cache frequently asked questions for better performance")
    print("   2. Use fuzzy matching for similar questions")
    print("   3. Implement context-aware response generation")
    print("   4. Rotate response templates to avoid detection")
    
    print("\n" + "="*80)
    print("âœ… QUESTION CACHING TEST COMPLETED SUCCESSFULLY!")
    print("="*80)
    
    return results

async def test_api_integration():
    """Test AI API integration for response generation"""
    print("\n" + "="*80)
    print("ğŸ¤– TESTING AI API INTEGRATION")
    print("="*80)
    
    # Check environment variables
    gemini_key = os.getenv('GEMINI_API_KEY')
    openai_key = os.getenv('OPENAI_API_KEY')
    
    print(f"ğŸ”‘ API Keys Status:")
    print(f"   Gemini: {'âœ… Available' if gemini_key else 'âŒ Not available'}")
    print(f"   OpenAI: {'âœ… Available' if openai_key else 'âŒ Not available'}")
    
    if not gemini_key and not openai_key:
        print("\nâš ï¸ No AI API keys found. Skipping API integration test.")
        print("   To test AI integration, set GEMINI_API_KEY or OPENAI_API_KEY environment variables.")
        return
    
    # Test API response generation
    test_questions = [
        "What motivates you to take surveys?",
        "How do you feel about earning money online?",
        "What's your experience with survey platforms?"
    ]
    
    print(f"\nğŸ§ª Testing AI response generation for {len(test_questions)} questions...")
    
    for i, question in enumerate(test_questions, 1):
        print(f"\n   Question {i}: {question}")
        
        try:
            # This would normally call the AI API
            # For now, we'll simulate the response
            if gemini_key:
                print(f"      Using Gemini API (simulated)")
            elif openai_key:
                print(f"      Using OpenAI API (simulated)")
            
            # Simulate API response
            await asyncio.sleep(0.5)
            print(f"      âœ… Response generated successfully")
            
        except Exception as e:
            print(f"      âŒ API error: {e}")
    
    print("\nğŸ¯ AI Integration Recommendations:")
    print("   1. Use multiple AI providers for redundancy")
    print("   2. Implement rate limiting to avoid API costs")
    print("   3. Cache AI responses to reduce API calls")
    print("   4. Use fallback templates when APIs are unavailable")

async def main():
    """Main test function"""
    print("ğŸš€ Starting Qubed Enhanced Bot Testing Suite")
    print("="*80)
    
    try:
        # Test question caching
        await test_question_caching()
        
        # Test API integration
        await test_api_integration()
        
        print("\nğŸ‰ All tests completed successfully!")
        
    except Exception as e:
        print(f"\nâŒ Test suite failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())
